# RANLP2025-Subtle-Shifts-Significant-Threats
This work investigates the vulnerability of pretrained English language models to stealthy adversarial attacks that subtly modify input text. The proposed approach leverages Explainable AI (XAI) methods to identify the most influential words driving model decisions and then replaces these words with contextually appropriate synonyms generated by small language models (SLMs). The attack aims to be minimal and semantically preserving while effectively undermining the robustness of language models. Experimental results across multiple datasets and models demonstrate that such targeted, subtle modifications significantly reduce classification performance, exposing inherent weaknesses and underscoring the need for improved defenses in natural language processing.

Please use the following citation:

```
@inproceedings{moreno-camara-2025-subtle,
    title = "Subtle Shifts, Significant Threats: Leveraging XAI Methods and LLMs to Undermine Language Models Robustness",
    author = "Moreno, Adri{\'a}n  and
      Ureña, L. Alfonso and
      Mart{\'i}nez, Eugenio",
    editor = "Mitkov, Ruslan  and
      Angelova, Galia",
    booktitle = "Proceedings of the 15th International Conference on Recent Advances in Natural Language Processing",
    month = sep,
    year = "2025",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd., Shoumen, Bulgaria",
    url = "(to appear)",
    pages = "(to appear)",
    abstract = "Language models exhibit inherent security vulnerabilities, which may be related to several factors, among them the malicious alteration of the input data. Such weaknesses compromise the robustness of language models, which is more critical when adversarial attacks are stealthy and do not require high computational resources. In this work, we study how vulnerable English language models are to adversarial attacks based on subtle modifications of the input of pretrained English language models. We claim that the attack may be more effective if it is targeted to the most salient words for the discriminative task of the language models. Accordingly, we propose a new attack built upon a two-step approach: first, we use a posteriori explainability methods to identify the most influential words for the classification task, and second, we replace them with contextual synonyms retrieved by a small language model. Since the attack has to be as stealthy as possible, we also propose a new evaluation measure that combines the effectiveness of the attack with the number of modifications performed. The results show that pretrained English language models are vulnerable to minimal semantic changes, which makes the design of countermeasure methods imperative."
}
```

> **Abstract:** Language models exhibit inherent security vulnerabilities, which may be related to several factors, among them the malicious alteration of the input data. Such weaknesses compromise the robustness of language models, which is more critical when adversarial attacks are stealthy and do not require high computational resources. In this work, we study how vulnerable English language models are to adversarial attacks based on subtle modifications of the input of pretrained English language models. We claim that the attack may be more effective if it is targeted to the most salient words for the discriminative task of the language models. Accordingly, we propose a new attack built upon a two-step approach: first, we use a posteriori explainability methods to identify the most influential words for the classification task, and second, we replace them with contextual synonyms retrieved by a small language model. Since the attack has to be as stealthy as possible, we also propose a new evaluation measure that combines the effectiveness of the attack with the number of modifications performed. The results show that pretrained English language models are vulnerable to minimal semantic changes, which makes the design of countermeasure methods imperative.

Contact person: Adrián Moreno Muñoz, ammunoz@ujaen.es

Don't hesitate to send us an e-mail or report an issue, if something is broken (and it shouldn't be) or if you have further questions.

> This repository contains experimental software and is published for the sole purpose of giving additional background details on the respective publication. 

### Example usage

``` bash
python lime_xai.py --model bert-base-cased --dataset sst2 --checkpoint best_bert-base_sst2 --config config.yaml

python captum_xai.py --model bert-base-cased --dataset sst2 --checkpoint best_bert-base_sst2 --config config.yaml

python shap_xai.py --model bert-base-cased --dataset sst2 --checkpoint best_bert-base_sst2 --config config.yaml

python attack.py --model_decoder gemma-3-1b-it --model_encoder bert-base-cased --dataset sst2 --attack_type shap
```

> **Additional Information:** In this setup, the decoder model was served using a dedicated inference server deployed with vLLM. This server efficiently hosts large language models providing improved memory usage and high throughput for handling inference requests. The script interacts with this vLLM server through the OpenAI API interface configured in the YAML settings.
